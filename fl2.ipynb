{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fl2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1itADT4cBkz-"
      },
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8Raa3myBzxd"
      },
      "source": [
        "inputs = np.load('/content/drive/MyDrive/inputs.npy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAaz38hGNfqC",
        "outputId": "ffa9c0dc-0a30-4b2c-dde3-4c4c35803a1a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_AFA7MU-x0D"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GRUCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, bias=True):\n",
        "        super(GRUCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bias = bias\n",
        "\n",
        "        # reset gate\n",
        "        self.fc_ir = nn.Linear(input_size, hidden_size, bias=bias)\n",
        "        self.fc_hr = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
        "\n",
        "        # update gate\n",
        "        self.fc_iz = nn.Linear(input_size, hidden_size, bias=bias)\n",
        "        self.fc_hz = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
        "\n",
        "        # new gate\n",
        "        self.fc_in = nn.Linear(input_size, hidden_size, bias=bias)\n",
        "        self.fc_hn = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
        "\n",
        "        self.init_parameters()\n",
        "\n",
        "    def init_parameters(self):\n",
        "        std = 1.0 / np.sqrt(self.hidden_size)\n",
        "        for w in self.parameters():\n",
        "            w.data.uniform_(-std, std)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "\n",
        "        x = x.view(-1, x.shape[1])\n",
        "\n",
        "        i_r = self.fc_ir(x)\n",
        "        h_r = self.fc_hr(h)\n",
        "        i_z = self.fc_iz(x)\n",
        "        h_z = self.fc_hz(h)\n",
        "        i_n = self.fc_in(x)\n",
        "        h_n = self.fc_hn(h)\n",
        "\n",
        "        resetgate = F.sigmoid(i_r + h_r)\n",
        "        inputgate = F.sigmoid(i_z + h_z)\n",
        "        newgate = F.tanh(i_n + (resetgate * h_n))\n",
        "\n",
        "        hy = newgate + inputgate * (h - newgate)\n",
        "\n",
        "        return hy\n",
        "\n",
        "\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size=1, embedding_dim=50, hidden_dim=10, bias=True, dropout=0.2):\n",
        "        super(GRU, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # GRU Cell\n",
        "        self.gru_cell = GRUCell(embedding_dim, hidden_dim)\n",
        "        # Fully-connected layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        # Sigmoid layer\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, h):\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # Deal with cases were the current batch_size is different from general batch_size\n",
        "        # It occurrs at the end of iteration with the Dataloaders\n",
        "        if h.shape[0] != batch_size:\n",
        "            h = h[:batch_size, :].contiguous()\n",
        "\n",
        "        # Apply embedding\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # GRU cells\n",
        "        for t in range(x.shape[1]):\n",
        "            h = self.gru_cell(x[:,t,:], h)\n",
        "\n",
        "        # Output corresponds to the last hidden state\n",
        "        out = h.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "        # Dropout and fully-connected layers\n",
        "        out = self.dropout(out)\n",
        "        sig_out = self.sigmoid(self.fc(out))\n",
        "\n",
        "        return sig_out, h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1XXKcA7-uWi"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "STOPWORDS = set([])  # set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = ' '.join([word for word in text.split() if word not in STOPWORDS])\n",
        "    return text\n",
        "\n",
        "\n",
        "def tokenize(text, word_to_idx):\n",
        "    tokens = []\n",
        "    for word in text.split():\n",
        "        tokens.append(word_to_idx[word])\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def pad_and_truncate(messages, max_length=30):\n",
        "    features = np.zeros((len(messages), max_length), dtype=int)\n",
        "    for i, sms in enumerate(messages):\n",
        "        if len(sms):\n",
        "            features[i, -len(sms):] = sms[:max_length]\n",
        "    return features\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    data = pd.read_csv('/content/drive/MyDrive/SMSSpamCollection.txt', sep='\\t', header=None, names=['label', 'sms'])\n",
        "    data.sms = data.sms.apply(clean_text)\n",
        "    words = set((' '.join(data.sms)).split())\n",
        "    word_to_idx = {word: i for i, word in enumerate(words, 1)}\n",
        "    tokens = data.sms.apply(lambda x: tokenize(x, word_to_idx))\n",
        "    inputs = pad_and_truncate(tokens)\n",
        "\n",
        "    labels = np.array((data.label == 'spam').astype(int))\n",
        "\n",
        "    np.save('/content/drive/MyDrive/labels.npy', labels)\n",
        "    np.save('/content/drive/MyDrive/inputs.npy', inputs)\n",
        "  \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6945Q03Dars"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GRUCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, bias=True):\n",
        "        super(GRUCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bias = bias\n",
        "\n",
        "        # reset gate\n",
        "        self.fc_ir = nn.Linear(input_size, hidden_size, bias=bias)\n",
        "        self.fc_hr = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
        "\n",
        "        # update gate\n",
        "        self.fc_iz = nn.Linear(input_size, hidden_size, bias=bias)\n",
        "        self.fc_hz = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
        "\n",
        "        # new gate\n",
        "        self.fc_in = nn.Linear(input_size, hidden_size, bias=bias)\n",
        "        self.fc_hn = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
        "\n",
        "        self.init_parameters()\n",
        "\n",
        "    def init_parameters(self):\n",
        "        std = 1.0 / np.sqrt(self.hidden_size)\n",
        "        for w in self.parameters():\n",
        "            w.data.uniform_(-std, std)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "\n",
        "        x = x.view(-1, x.shape[1])\n",
        "\n",
        "        i_r = self.fc_ir(x)\n",
        "        h_r = self.fc_hr(h)\n",
        "        i_z = self.fc_iz(x)\n",
        "        h_z = self.fc_hz(h)\n",
        "        i_n = self.fc_in(x)\n",
        "        h_n = self.fc_hn(h)\n",
        "\n",
        "        resetgate = F.sigmoid(i_r + h_r)\n",
        "        inputgate = F.sigmoid(i_z + h_z)\n",
        "        newgate = F.tanh(i_n + (resetgate * h_n))\n",
        "\n",
        "        hy = newgate + inputgate * (h - newgate)\n",
        "\n",
        "        return hy\n",
        "\n",
        "\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size=1, embedding_dim=50, hidden_dim=10, bias=True, dropout=0.2):\n",
        "        super(GRU, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # GRU Cell\n",
        "        self.gru_cell = GRUCell(embedding_dim, hidden_dim)\n",
        "        # Fully-connected layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        # Sigmoid layer\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, h):\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # Deal with cases were the current batch_size is different from general batch_size\n",
        "        # It occurrs at the end of iteration with the Dataloaders\n",
        "        if h.shape[0] != batch_size:\n",
        "            h = h[:batch_size, :].contiguous()\n",
        "\n",
        "        # Apply embedding\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # GRU cells\n",
        "        for t in range(x.shape[1]):\n",
        "            h = self.gru_cell(x[:,t,:], h)\n",
        "\n",
        "        # Output corresponds to the last hidden state\n",
        "        out = h.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "        # Dropout and fully-connected layers\n",
        "        out = self.dropout(out)\n",
        "        sig_out = self.sigmoid(self.fc(out))\n",
        "\n",
        "        return sig_out, h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzsMXGiuETJY"
      },
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fHw2F5nEdXv"
      },
      "source": [
        "inputs = np.load('/content/drive/MyDrive/inputs.npy')\n",
        "labels = np.load('/content/drive/MyDrive/labels.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLdd4f2YEtf9"
      },
      "source": [
        "VOCAB_SIZE = int(inputs.max()) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJwswfYyEwPb"
      },
      "source": [
        "# Training params\n",
        "EPOCHS = 15\n",
        "CLIP = 5 # gradient clipping - to avoid gradient explosion (frequent in RNNs)\n",
        "lr = 0.1\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Model params\n",
        "EMBEDDING_DIM = 50\n",
        "HIDDEN_DIM = 10\n",
        "DROPOUT = 0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xs5-t9wAExtt",
        "outputId": "70fef934-bc82-435c-a37e-09b8567f944b"
      },
      "source": [
        "pip install pysyft"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pysyft in /usr/local/lib/python3.7/dist-packages (0.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "PlDYEvJ2HCmD",
        "outputId": "aa910e5d-a7db-4111-9f56-a26562964330"
      },
      "source": [
        "from syft.frameworks.torch.dp import pate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-c12cfcd00d71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msyft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeworks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'syft'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hVYmKsbeHhN3",
        "outputId": "cf277dfb-a783-4bf1-bb42-c6126965abba"
      },
      "source": [
        "!pip install syft==0.2.9"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting syft==0.2.9\n",
            "  Downloading syft-0.2.9-py3-none-any.whl (433 kB)\n",
            "\u001b[K     |████████████████████████████████| 433 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting tblib~=1.6.0\n",
            "  Downloading tblib-1.6.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting notebook==5.7.8\n",
            "  Downloading notebook-5.7.8-py2.py3-none-any.whl (9.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0 MB 26.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (7.1.2)\n",
            "Collecting aiortc==0.9.28\n",
            "  Downloading aiortc-0.9.28-cp37-cp37m-manylinux2010_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 43.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill~=0.3.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (0.3.4)\n",
            "Requirement already satisfied: msgpack~=1.0.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (1.0.2)\n",
            "Collecting syft-proto~=0.5.2\n",
            "  Downloading syft_proto-0.5.3-py3-none-any.whl (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting requests~=2.22.0\n",
            "  Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting numpy~=1.18.1\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 76.0 MB/s \n",
            "\u001b[?25hCollecting websockets~=8.1.0\n",
            "  Downloading websockets-8.1-cp37-cp37m-manylinux2010_x86_64.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.5 MB/s \n",
            "\u001b[?25hCollecting importlib-resources~=1.5.0\n",
            "  Downloading importlib_resources-1.5.0-py2.py3-none-any.whl (21 kB)\n",
            "Collecting torchvision~=0.5.0\n",
            "  Downloading torchvision-0.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 58.6 MB/s \n",
            "\u001b[?25hCollecting psutil==5.7.0\n",
            "  Downloading psutil-5.7.0.tar.gz (449 kB)\n",
            "\u001b[K     |████████████████████████████████| 449 kB 59.0 MB/s \n",
            "\u001b[?25hCollecting shaloop==0.2.1-alpha.11\n",
            "  Downloading shaloop-0.2.1_alpha.11-py3-none-manylinux1_x86_64.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 55.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy~=1.4.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (1.4.1)\n",
            "Requirement already satisfied: Flask~=1.1.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (1.1.4)\n",
            "Collecting flask-socketio~=4.2.1\n",
            "  Downloading Flask_SocketIO-4.2.1-py2.py3-none-any.whl (16 kB)\n",
            "Collecting RestrictedPython~=5.0\n",
            "  Downloading RestrictedPython-5.2-py2.py3-none-any.whl (28 kB)\n",
            "Collecting requests-toolbelt==0.9.1\n",
            "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.8 MB/s \n",
            "\u001b[?25hCollecting lz4~=3.0.2\n",
            "  Downloading lz4-3.0.2-cp37-cp37m-manylinux2010_x86_64.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 59.7 MB/s \n",
            "\u001b[?25hCollecting openmined.threepio==0.2.0\n",
            "  Downloading openmined.threepio-0.2.0.tar.gz (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting phe~=1.4.0\n",
            "  Downloading phe-1.4.0.tar.gz (35 kB)\n",
            "Collecting torch~=1.4.0\n",
            "  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4 MB 5.5 kB/s \n",
            "\u001b[?25hCollecting tornado==4.5.3\n",
            "  Downloading tornado-4.5.3.tar.gz (484 kB)\n",
            "\u001b[K     |████████████████████████████████| 484 kB 51.0 MB/s \n",
            "\u001b[?25hCollecting websocket-client~=0.57.0\n",
            "  Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[K     |████████████████████████████████| 200 kB 55.9 MB/s \n",
            "\u001b[?25hCollecting pyee>=6.0.0\n",
            "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
            "Collecting av<9.0.0,>=8.0.0\n",
            "  Downloading av-8.0.3-cp37-cp37m-manylinux2010_x86_64.whl (37.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 37.2 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from aiortc==0.9.28->syft==0.2.9) (1.15.0)\n",
            "Collecting crc32c\n",
            "  Downloading crc32c-2.2.post0-cp37-cp37m-manylinux2010_x86_64.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.2\n",
            "  Downloading cryptography-36.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 45.1 MB/s \n",
            "\u001b[?25hCollecting pylibsrtp>=0.5.6\n",
            "  Downloading pylibsrtp-0.6.8-cp37-cp37m-manylinux2010_x86_64.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting aioice<0.7.0,>=0.6.17\n",
            "  Downloading aioice-0.6.18-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (2.11.3)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (4.9.1)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (5.1.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (0.12.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (5.6.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (0.12.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (4.10.1)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (22.3.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (1.8.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (0.2.0)\n",
            "Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (5.3.5)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (5.1.3)\n",
            "Requirement already satisfied: pycparser>=2 in /usr/local/lib/python3.7/dist-packages (from shaloop==0.2.1-alpha.11->syft==0.2.9) (2.21)\n",
            "Collecting netifaces\n",
            "  Downloading netifaces-0.11.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (32 kB)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.1->syft==0.2.9) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.1->syft==0.2.9) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.1->syft==0.2.9) (1.1.0)\n",
            "Collecting python-socketio>=4.3.0\n",
            "  Downloading python_socketio-5.5.0-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from importlib-resources~=1.5.0->syft==0.2.9) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.4 in /usr/local/lib/python3.7/dist-packages (from importlib-resources~=1.5.0->syft==0.2.9) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook==5.7.8->syft==0.2.9) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=5.2.0->notebook==5.7.8->syft==0.2.9) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->jupyter-client>=5.2.0->notebook==5.7.8->syft==0.2.9) (1.15.0)\n",
            "Collecting python-engineio>=4.3.0\n",
            "  Downloading python_engineio-4.3.0-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting bidict>=0.21.0\n",
            "  Downloading bidict-0.21.4-py3-none-any.whl (36 kB)\n",
            "Collecting idna<2.9,>=2.5\n",
            "  Downloading idna-2.8-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests~=2.22.0->syft==0.2.9) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests~=2.22.0->syft==0.2.9) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests~=2.22.0->syft==0.2.9) (2021.10.8)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from syft-proto~=0.5.2->syft==0.2.9) (3.17.3)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook==5.7.8->syft==0.2.9) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->importlib-resources~=1.5.0->syft==0.2.9) (3.10.0.2)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->notebook==5.7.8->syft==0.2.9) (5.5.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (57.4.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (4.4.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (0.2.5)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (0.5.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (0.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (1.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (4.1.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook==5.7.8->syft==0.2.9) (2.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook==5.7.8->syft==0.2.9) (21.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook==5.7.8->syft==0.2.9) (0.5.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->bleach->nbconvert->notebook==5.7.8->syft==0.2.9) (3.0.6)\n",
            "Building wheels for collected packages: openmined.threepio, psutil, tornado, phe\n",
            "  Building wheel for openmined.threepio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openmined.threepio: filename=openmined.threepio-0.2.0-py3-none-any.whl size=80095 sha256=95eaff0880a4ad1bc97c93e57844935ed52a594e40a9a2bf748cfa54edc4bf6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/3d/ce/4ca4386006e622cb87d5116e5e65026ec021d3cf906a9b3d5d\n",
            "  Building wheel for psutil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for psutil: filename=psutil-5.7.0-cp37-cp37m-linux_x86_64.whl size=276500 sha256=0593658143f2defb30c8ad1339b45b8db53811ac941fd31576a4a64b7970db24\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/e7/50/aee9cc966163d74430f13f208171dee22f11efa4a4a826661c\n",
            "  Building wheel for tornado (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tornado: filename=tornado-4.5.3-cp37-cp37m-linux_x86_64.whl size=434053 sha256=c4d06b4ee538e9218c406f6963d91fc6eeff886c6c6741d4fbd53d26ae438d71\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/45/43/36ec7a893e16c1212a6b1505ded0a2d73cf8e863a0227c8e04\n",
            "  Building wheel for phe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for phe: filename=phe-1.4.0-py2.py3-none-any.whl size=37362 sha256=56a70171a12cf1a0abd1cfb0181bdb590f8387925744a30278c378b1d18faa11\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/ac/9b/b07a04fe6bb1418ab4ee06d6652757aef848b80363c4dac507\n",
            "Successfully built openmined.threepio psutil tornado phe\n",
            "Installing collected packages: tornado, python-engineio, netifaces, idna, bidict, torch, requests, python-socketio, pylibsrtp, pyee, numpy, cryptography, crc32c, av, aioice, websockets, websocket-client, torchvision, tblib, syft-proto, shaloop, RestrictedPython, requests-toolbelt, psutil, phe, openmined.threepio, notebook, lz4, importlib-resources, flask-socketio, aiortc, syft\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "  Attempting uninstall: tblib\n",
            "    Found existing installation: tblib 1.7.0\n",
            "    Uninstalling tblib-1.7.0:\n",
            "      Successfully uninstalled tblib-1.7.0\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Attempting uninstall: notebook\n",
            "    Found existing installation: notebook 5.3.1\n",
            "    Uninstalling notebook-5.3.1:\n",
            "      Successfully uninstalled notebook-5.3.1\n",
            "  Attempting uninstall: importlib-resources\n",
            "    Found existing installation: importlib-resources 5.4.0\n",
            "    Uninstalling importlib-resources-5.4.0:\n",
            "      Successfully uninstalled importlib-resources-5.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.4.0 which is incompatible.\n",
            "google-colab 1.0.0 requires notebook~=5.3.0; python_version >= \"3.0\", but you have notebook 5.7.8 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.22.0 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 4.5.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "bokeh 2.3.3 requires tornado>=5.1, but you have tornado 4.5.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed RestrictedPython-5.2 aioice-0.6.18 aiortc-0.9.28 av-8.0.3 bidict-0.21.4 crc32c-2.2.post0 cryptography-36.0.0 flask-socketio-4.2.1 idna-2.8 importlib-resources-1.5.0 lz4-3.0.2 netifaces-0.11.0 notebook-5.7.8 numpy-1.18.5 openmined.threepio-0.2.0 phe-1.4.0 psutil-5.7.0 pyee-8.2.2 pylibsrtp-0.6.8 python-engineio-4.3.0 python-socketio-5.5.0 requests-2.22.0 requests-toolbelt-0.9.1 shaloop-0.2.1a11 syft-0.2.9 syft-proto-0.5.3 tblib-1.6.0 torch-1.4.0 torchvision-0.5.0 tornado-4.5.3 websocket-client-0.57.0 websockets-8.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "psutil",
                  "torch",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjsFHg8PGjWw"
      },
      "source": [
        "import syft as sy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpXuCuphJDjD"
      },
      "source": [
        "labels = torch.tensor(labels)\n",
        "inputs = torch.tensor(inputs)\n",
        "\n",
        "# splitting training and test data\n",
        "pct_test = 0.2\n",
        "\n",
        "train_labels = labels[:-int(len(labels)*pct_test)]\n",
        "train_inputs = inputs[:-int(len(labels)*pct_test)]\n",
        "\n",
        "test_labels = labels[-int(len(labels)*pct_test):]\n",
        "test_inputs = inputs[-int(len(labels)*pct_test):]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ9NKNd1JE9Q"
      },
      "source": [
        "# Hook that extends the Pytorch library to enable all computations with pointers of tensors sent to other workers\n",
        "hook = sy.TorchHook(torch)\n",
        "\n",
        "# Creating 2 virtual workers\n",
        "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
        "anne = sy.VirtualWorker(hook, id=\"anne\")\n",
        "\n",
        "# threshold indexes for dataset split (one half for Bob, other half for Anne)\n",
        "train_idx = int(len(train_labels)/2)\n",
        "test_idx = int(len(test_labels)/2)\n",
        "\n",
        "# Sending toy datasets to virtual workers\n",
        "bob_train_dataset = sy.BaseDataset(train_inputs[:train_idx], train_labels[:train_idx]).send(bob)\n",
        "anne_train_dataset = sy.BaseDataset(train_inputs[train_idx:], train_labels[train_idx:]).send(anne)\n",
        "bob_test_dataset = sy.BaseDataset(test_inputs[:test_idx], test_labels[:test_idx]).send(bob)\n",
        "anne_test_dataset = sy.BaseDataset(test_inputs[test_idx:], test_labels[test_idx:]).send(anne)\n",
        "\n",
        "# Creating federated datasets, an extension of Pytorch TensorDataset class\n",
        "federated_train_dataset = sy.FederatedDataset([bob_train_dataset, anne_train_dataset])\n",
        "federated_test_dataset = sy.FederatedDataset([bob_test_dataset, anne_test_dataset])\n",
        "\n",
        "# Creating federated dataloaders, an extension of Pytorch DataLoader class\n",
        "federated_train_loader = sy.FederatedDataLoader(federated_train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
        "federated_test_loader = sy.FederatedDataLoader(federated_test_dataset, shuffle=False, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpGg7gFRKiNm",
        "outputId": "0cb8ed99-946b-47d5-bedd-f95ae3e81aec"
      },
      "source": [
        "%%writefile handcrafted_GRU.py\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GRUCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, bias=True):\n",
        "        super(GRUCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bias = bias\n",
        "\n",
        "        # reset gate\n",
        "        self.fc_ir = nn.Linear(input_size, hidden_size, bias=bias)\n",
        "        self.fc_hr = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
        "\n",
        "        # update gate\n",
        "        self.fc_iz = nn.Linear(input_size, hidden_size, bias=bias)\n",
        "        self.fc_hz = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
        "\n",
        "        # new gate\n",
        "        self.fc_in = nn.Linear(input_size, hidden_size, bias=bias)\n",
        "        self.fc_hn = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
        "\n",
        "        self.init_parameters()\n",
        "\n",
        "    def init_parameters(self):\n",
        "        std = 1.0 / np.sqrt(self.hidden_size)\n",
        "        for w in self.parameters():\n",
        "            w.data.uniform_(-std, std)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "\n",
        "        x = x.view(-1, x.shape[1])\n",
        "\n",
        "        i_r = self.fc_ir(x)\n",
        "        h_r = self.fc_hr(h)\n",
        "        i_z = self.fc_iz(x)\n",
        "        h_z = self.fc_hz(h)\n",
        "        i_n = self.fc_in(x)\n",
        "        h_n = self.fc_hn(h)\n",
        "\n",
        "        resetgate = F.sigmoid(i_r + h_r)\n",
        "        inputgate = F.sigmoid(i_z + h_z)\n",
        "        newgate = F.tanh(i_n + (resetgate * h_n))\n",
        "\n",
        "        hy = newgate + inputgate * (h - newgate)\n",
        "\n",
        "        return hy\n",
        "\n",
        "\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size=1, embedding_dim=50, hidden_dim=10, bias=True, dropout=0.2):\n",
        "        super(GRU, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # GRU Cell\n",
        "        self.gru_cell = GRUCell(embedding_dim, hidden_dim)\n",
        "        # Fully-connected layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        # Sigmoid layer\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, h):\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # Deal with cases were the current batch_size is different from general batch_size\n",
        "        # It occurrs at the end of iteration with the Dataloaders\n",
        "        if h.shape[0] != batch_size:\n",
        "            h = h[:batch_size, :].contiguous()\n",
        "\n",
        "        # Apply embedding\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # GRU cells\n",
        "        for t in range(x.shape[1]):\n",
        "            h = self.gru_cell(x[:,t,:], h)\n",
        "\n",
        "        # Output corresponds to the last hidden state\n",
        "        out = h.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "        # Dropout and fully-connected layers\n",
        "        out = self.dropout(out)\n",
        "        sig_out = self.sigmoid(self.fc(out))\n",
        "\n",
        "        return sig_out, h"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing handcrafted_GRU.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhwmkRliLpHG",
        "outputId": "c45707ed-ae7c-4944-ac32-c6f95630a617"
      },
      "source": [
        "!pip install handcrafted_GRU"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement handcrafted_GRU (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for handcrafted_GRU\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psyuX-VaQKYT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnXUzmp5Olni"
      },
      "source": [
        "import handcrafted_GRU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_RzYFtvJJRF"
      },
      "source": [
        "from handcrafted_GRU import GRU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIHT4YWZRMvI"
      },
      "source": [
        "# Initiating the model\n",
        "model = GRU(vocab_size=VOCAB_SIZE, hidden_dim=HIDDEN_DIM, embedding_dim=EMBEDDING_DIM, dropout=DROPOUT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NjTr60zReZU"
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bZOYzRzRSuG",
        "outputId": "8b5940f7-a2bc-46ba-d505-21ac1d4795d5"
      },
      "source": [
        "for e in range(EPOCHS):\n",
        "    \n",
        "    ######### Training ##########\n",
        "    \n",
        "    losses = []\n",
        "    # Batch loop\n",
        "    for inputs, labels in federated_train_loader:\n",
        "        # Location of current batch\n",
        "        worker = inputs.location\n",
        "        # Initialize hidden state and send it to worker\n",
        "        h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM))).send(worker)\n",
        "        # Send model to current worker\n",
        "        model.send(worker)\n",
        "        # Setting accumulated gradients to zero before backward step\n",
        "        optimizer.zero_grad()\n",
        "        # Output from the model\n",
        "        output, _ = model(inputs, h)\n",
        "        # Calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        # Clipping the gradient to avoid explosion\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "        # Backpropagation step\n",
        "        optimizer.step() \n",
        "        # Get the model back to the local worker\n",
        "        model.get()\n",
        "        losses.append(loss.get())\n",
        "    \n",
        "    ######## Evaluation ##########\n",
        "    \n",
        "    # Model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_preds = []\n",
        "        test_labels_list = []\n",
        "        eval_losses = []\n",
        "\n",
        "        for inputs, labels in federated_test_loader:\n",
        "            # get current location\n",
        "            worker = inputs.location\n",
        "            # Initialize hidden state and send it to worker\n",
        "            h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM))).send(worker)    \n",
        "            # Send model to worker\n",
        "            model.send(worker)\n",
        "            \n",
        "            output, _ = model(inputs, h)\n",
        "            loss = criterion(output.squeeze(), labels.float())\n",
        "            eval_losses.append(loss.get())\n",
        "            preds = output.squeeze().get()\n",
        "            test_preds += list(preds.numpy())\n",
        "            test_labels_list += list(labels.get().numpy().astype(int))\n",
        "            # Get the model back to the local worker\n",
        "            model.get()\n",
        "        \n",
        "        score = roc_auc_score(test_labels_list, test_preds)\n",
        "    \n",
        "    print(\"Epoch {}/{}...  \\\n",
        "    AUC: {:.3%}...  \\\n",
        "    Training loss: {:.5f}...  \\\n",
        "    Validation loss: {:.5f}\".format(e+1, EPOCHS, score, sum(losses)/len(losses), sum(eval_losses)/len(eval_losses)))\n",
        "    \n",
        "    model.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function ObjectPointer.__del__ at 0x7f75b00439e0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/syft/generic/pointers/object_pointer.py\", line 345, in __del__\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/syft/workers/base.py\", line 480, in garbage\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/syft/workers/base.py\", line 316, in send_msg\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/syft/workers/virtual.py\", line 12, in _send_msg\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/syft/workers/virtual.py\", line 22, in _recv_msg\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/syft/workers/base.py\", line 356, in recv_msg\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/syft/generic/abstract/message_handler.py\", line 20, in handle\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/syft/workers/message_handler.py\", line 211, in handle_force_delete_object_msg\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/syft/generic/object_storage.py\", line 133, in force_rm_obj\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/syft/generic/object_storage.py\", line 130, in rm_obj\n",
            "KeyError: 37662561715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15...      AUC: 69.634%...      Training loss: 0.45206...      Validation loss: 0.36811\n",
            "Epoch 2/15...      AUC: 78.354%...      Training loss: 0.36397...      Validation loss: 0.33390\n",
            "Epoch 3/15...      AUC: 82.994%...      Training loss: 0.32387...      Validation loss: 0.29895\n",
            "Epoch 4/15...      AUC: 86.983%...      Training loss: 0.28711...      Validation loss: 0.26503\n",
            "Epoch 5/15...      AUC: 91.005%...      Training loss: 0.24304...      Validation loss: 0.23026\n",
            "Epoch 6/15...      AUC: 93.833%...      Training loss: 0.19603...      Validation loss: 0.18325\n",
            "Epoch 7/15...      AUC: 95.602%...      Training loss: 0.15561...      Validation loss: 0.15088\n",
            "Epoch 8/15...      AUC: 96.313%...      Training loss: 0.12928...      Validation loss: 0.13119\n",
            "Epoch 9/15...      AUC: 96.681%...      Training loss: 0.11009...      Validation loss: 0.12086\n",
            "Epoch 10/15...      AUC: 96.784%...      Training loss: 0.09602...      Validation loss: 0.11356\n",
            "Epoch 11/15...      AUC: 96.794%...      Training loss: 0.08433...      Validation loss: 0.11257\n",
            "Epoch 12/15...      AUC: 96.976%...      Training loss: 0.07075...      Validation loss: 0.10758\n",
            "Epoch 13/15...      AUC: 97.125%...      Training loss: 0.06529...      Validation loss: 0.10776\n",
            "Epoch 14/15...      AUC: 97.045%...      Training loss: 0.05252...      Validation loss: 0.10547\n",
            "Epoch 15/15...      AUC: 97.391%...      Training loss: 0.04825...      Validation loss: 0.10186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "K0Lp12HZY26W",
        "outputId": "cc23f606-63c5-4712-a905-4be468013bf5"
      },
      "source": [
        "for e in range(EPOCHS):\n",
        "    \n",
        "    ######### Training ##########\n",
        "    \n",
        "    losses = []\n",
        "    # Batch loop\n",
        "    for inputs, labels in federated_train_loader:\n",
        "        # Location of current batch\n",
        "        worker = inputs.location\n",
        "        # Initialize hidden state and send it to worker\n",
        "        h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM))).send(worker)\n",
        "        # Send model to current worker\n",
        "        model.send(worker)\n",
        "        # Setting accumulated gradients to zero before backward step\n",
        "        optimizer.zero_grad()\n",
        "        # Output from the model\n",
        "        output, _ = model(inputs, h)\n",
        "        # Calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        # Clipping the gradient to avoid explosion\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "        # Backpropagation step\n",
        "        optimizer.step() \n",
        "        # Get the model back to the local worker\n",
        "        model.get()\n",
        "        losses.append(loss.get())\n",
        "    \n",
        "    ######## Evaluation ##########\n",
        "    \n",
        "    # Model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_preds = []\n",
        "        test_labels_list = []\n",
        "        eval_losses = []\n",
        "\n",
        "        for inputs, labels in federated_test_loader:\n",
        "            # get current location\n",
        "            worker = inputs.location\n",
        "            # Initialize hidden state and send it to worker\n",
        "            h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM))).send(worker)    \n",
        "            # Send model to worker\n",
        "            model.send(worker)\n",
        "            \n",
        "            output, _ = model(inputs, h)\n",
        "            loss = criterion(output.squeeze(), labels.float())\n",
        "            eval_losses.append(loss.get())\n",
        "            preds = output.squeeze().get()\n",
        "            test_preds += list(preds.numpy())\n",
        "            test_labels_list += list(labels.get().numpy().astype(int))\n",
        "            # Get the model back to the local worker\n",
        "            model.get()\n",
        "        \n",
        "        score = roc_auc_score(test_labels_list, test_preds)\n",
        "        metrics.accuracy_score(test_labels_list, test_preds)\n",
        "        \n",
        "\n",
        "    \n",
        "    print(\"Epoch {}/{}...  \\\n",
        "    AUC: {:.3%}...  \\\n",
        "    Training loss: {:.5f}...  \\\n",
        "    Validation loss: {:.5f}\".format(e+1, EPOCHS, score, sum(losses)/len(losses), sum(eval_losses)/len(eval_losses)))\n",
        "    \n",
        "    model.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-bff5c34e931d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
          ]
        }
      ]
    }
  ]
}